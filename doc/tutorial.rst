LCE Presentation
================
As shown in “Why Do Tree-Based Models still Outperform Deep Learning on Tabular Data?” [8]_, **the widely used tree-based models remain the state-of-the-art machine learning methods in many cases**. 
**Local Cascade Ensemble (LCE)** [7]_ proposes to combine the strengths of the top performing tree-based ensemble methods - Random Forest [3]_ and eXtreme Gradient Boosting (XGBoost) [4]_, 
and integrates a supplementary diversification approach which enables it to be **a better generalizing predictor**.

Overview
--------
The construction of an ensemble method involves combining accurate and diverse individual predictors. 
There are **two complementary ways** to generate diverse predictors: *(i)* by **changing the training data distribution** and *(ii)* by **learning different parts of the training data**.

**LCE adopts these two diversification approaches.** 
First, *(i)* LCE combines the two well-known methods that modify the distribution of the original training data with complementary effects on the bias-variance trade-off: bagging [2]_ (variance reduction) and boosting [11]_ (bias reduction). 
Then, *(ii)* LCE learns different parts of the training data to capture new relationships that cannot be discovered globally based on a divide-and-conquer strategy (a decision tree). 
Before detailing how LCE combines these methods, we introduce the key concepts behind them that will be used in the explanation of LCE.

Concepts
--------
The bias-variance trade-off defines the capacity of the learning algorithm to generalize beyond the training set. 
The *bias* is the component of the prediction error that results from systematic errors of the learning algorithm. 
A high bias means that the learning algorithm is not able to capture the underlying structure of the training set (underfitting). 
The *variance* measures the sensitivity of the learning algorithm to changes in the training set. 
A high variance means that the algorithm is learning too closely the training set (overfitting). 
The objective is to minimize both the bias and variance. *Bagging* has a main effect on variance reduction; it is a method for generating multiple versions of a predictor (bootstrap replicates) and using these to get an aggregated predictor. 
The current state-of-the-art method that employs bagging is Random Forest [3]_. 
Whereas, *boosting* has a main effect on bias reduction; it is a method for iteratively learning weak predictors and adding them to create a final strong one. 
After a weak learner is added, the data weights are readjusted, allowing future weak learners to focus more on the examples that previous weak learners mispredicted. 
The current state-of-the-art method that uses boosting is XGBoost [4]_. 
The following Figure illustrates the difference between bagging and boosting methods.


.. image:: _images/Figure_BaggingvsBoosting.png
   :width: 90%
   :align: center
  

LCE 
---
LCE combines a boosting-bagging approach to handle the bias-variance trade-off faced by machine learning models; in addition, it adopts a divide-and-conquer approach to individualize predictor errors on different parts of the training data. 
LCE is represented in the following Figure.

.. image:: _images/Figure_LCE.png
   :width: 90%
   :align: center


Specifically, LCE is based on cascade generalization: it uses a set of predictors sequentially, and adds new attributes to the input dataset at each stage. 
The new attributes are derived from the output given by a predictor (e.g., class probabilities for a classifier), called a base learner. 
LCE applies cascade generalization locally following a divide-and-conquer strategy - a decision tree, and reduces bias across a decision tree through the use of boosting-based predictors as base learners. 
The current best performing state-of-the-art boosting algorithm is adopted as base learner by default (XGBoost, e.g., XGB¹°, XGB¹¹ in above Figure). 
CatBoost [10]_ and LightGBM [9]_ can also be chosen as base learner.
When growing the tree, boosting is propagated down the tree by adding the output of the base learner at each decision node as new attributes to the dataset (e.g., XGB¹°(D¹) in above Figure). 
Prediction outputs indicate the ability of the base learner to correctly predict a sample. 
At the next tree level, the outputs added to the dataset are exploited by the base learner as a weighting scheme to focus more on previously mispredicted samples. 
Then, the overfitting generated by the boosted decision tree is mitigated by the use of bagging. 
Bagging provides variance reduction by creating multiple predictors from random sampling with replacement of the original dataset (e.g., D¹, D² in above Figure). 
Finally, trees are aggregated with a simple majority vote. 
In order to be applied as a predictor, LCE stores, in each node, the model generated by the base learner.

Missing Data
------------
LCE natively handles missing data. 
Similar to XGBoost, LCE excludes missing values for the split and uses block propagation. 
During a node split, block propagation sends all samples with missing data to the side of the decision node with less errors.

Hyperparameters
---------------
The hyperparameters of LCE are the classical ones in tree-based learning (e.g., ``max_depth``, ``max_features``, ``n_estimators``). 
Moreover, LCE learns a specific XGBoost model at each node of a tree, and it only requires the ranges of XGBoost hyperparameters to be specified. 
Then, the hyperparameters of each XGBoost model are automatically set by Hyperopt [1]_, a sequential model-based optimization using a tree of Parzen estimators algorithm. 
Hyperopt chooses the next hyperparameters from both the previous choices and a tree-based optimization algorithm. 
Tree of Parzen estimators meets or exceeds grid search and random search performance for hyperparameters setting. 
The full list of LCE hyperparameters is available in its :ref:`API documentation <APIDocumentation>`.

Published Results
-----------------
LCE has been initially designed for a specific application in [6]_, and then evaluated on the public UCI datasets [5]_ in [7]_. 
Results show that LCE obtains on average a better prediction performance than the state-of-the-art classifiers, including Random Forest and XGBoost.
For a comparison between LCE, Random Forest and XGBoost on different public datasets, using the public implementations of the aforementioned algorithms, please refer to the article published in Towards Data Science `"LCE: The Most Powerful Machine Learning Method?" <https://towardsdatascience.com/lce-the-most-powerful-machine-learning-method-e8ea77f317d6?source=friends_link&sk=c8911ad03dd1e0e3fd02a17835609737>`_.


References
----------
.. [1] Bergstra, J., R. Bardenet, Y. Bengio and B. Kégl. Algorithms for Hyper-Parameter Optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems, 2011
.. [2] Breiman, L. Bagging Predictors. Machine Learning, 24(2):123–140, 1996
.. [3] Breiman, L. Random Forests. Machine Learning, 45(1):5–32, 2001
.. [4] Chen, T. and C. Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016
.. [5] Dua, D. and C. Graff. UCI Machine Learning Repository, 2017
.. [6] Fauvel, K., V. Masson, E. Fromont, P. Faverdin and A. Termier. Towards Sustainable Dairy Management - A Machine Learning Enhanced Method for Estrus Detection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019
.. [7] Fauvel, K., E. Fromont, V. Masson, P. Faverdin and A. Termier. XEM: An Explainable-by-Design Ensemble Method for Multivariate Time Series Classification. Data Mining and Knowledge Discovery, 36(3):917–957, 2022
.. [8] Grinsztajn, L., E. Oyallon and G. Varoquaux. Why Do Tree-Based Models still Outperform Deep Learning on Typical Tabular Data? In Proceedings of the 36th Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022
.. [9] Ke, G., Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and T. Liu. LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017
.. [10] Prokhorenkova, L., G. Gusev, A. Vorobev, A. Dorogush and A. Gulin. CatBoost: Unbiased Boosting with Categorical Features. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018
.. [11] Schapire, R. The Strength of Weak Learnability. Machine Learning, 5(2):197–227, 1990



Installation
============

You can install LCE from `PyPI <https://pypi.org/project/lcensemble/>`_ with ``pip``::

	pip install lcensemble
	
Or ``conda``::

	conda install -c conda-forge lcensemble


Code Examples
=============

The following examples illustrate the use of LCE on public datasets for a classification and a regression task.
They also demonstrate the compatibility of LCE with scikit-learn pipelines and model selection tools through the use of ``cross_val_score``.
An example of LCE on a dataset including missing values is also shown.

Classification
--------------

- **Example 1: LCE on Iris Dataset**

.. code-block:: python

	from lce import LCEClassifier
	from sklearn.datasets import load_iris
	from sklearn.metrics import accuracy_score
	from sklearn.model_selection import train_test_split


	# Load data and generate a train/test split
	data = load_iris()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Train LCEClassifier with default parameters
	clf = LCEClassifier(n_jobs=-1, random_state=0)
	clf.fit(X_train, y_train)

	# Make prediction and compute accuracy score
	y_pred = clf.predict(X_test)
	accuracy = accuracy_score(y_test, y_pred)
	print("Accuracy: {:.1f}%".format(accuracy*100))
   
.. code-block::
	
	Accuracy: 97.4%


- **Example 2: LCE with scikit-learn cross validation score**
This example demonstrates the compatibility of LCE with scikit-learn pipelines and model selection tools through the use of ``cross_val_score``.

.. code-block:: python

	from lce import LCEClassifier
	from sklearn.datasets import load_iris
	from sklearn.model_selection import cross_val_score, train_test_split

	# Load data
	data = load_iris()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Set LCEClassifier with default parameters
	clf = LCEClassifier(n_jobs=-1, random_state=0)

	# Compute cross-validation scores
	cv_scores = cross_val_score(clf, X_train, y_train, cv=3)
	cv_scores = [round(elem*100, 1) for elem in cv_scores.tolist()]
	print("Cross-validation scores on train set: ", cv_scores)
   
.. code-block::
	
	Cross-validation scores on train set:  [94.7, 100.0, 94.6]


Regression
----------

- **Example 3: LCE on Diabetes Dataset**

.. code-block:: python

	from lce import LCERegressor
	from sklearn.datasets import load_diabetes
	from sklearn.metrics import mean_squared_error
	from sklearn.model_selection import train_test_split


	# Load data and generate a train/test split
	data = load_diabetes()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Train LCERegressor with default parameters
	reg = LCERegressor(n_jobs=-1, random_state=0)
	reg.fit(X_train, y_train)

	# Make prediction 
	y_pred = reg.predict(X_test)
	mse = mean_squared_error(y_test, y_pred)
	print("The mean squared error (MSE) on test set: {:.0f}".format(mse))
	
.. code-block::
	
	The mean squared error (MSE) on test set: 3761
	  

- **Example 4: LCE with missing values**
This example illustrates the robustness of LCE to missing values. The Diabetes train set is modified with 20% of missing values per variable.

.. code-block:: python

	import numpy as np
	from lce import LCERegressor
	from sklearn.datasets import load_diabetes
	from sklearn.metrics import mean_squared_error
	from sklearn.model_selection import train_test_split


	# Load data and generate a train/test split
	data = load_diabetes()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Input 20% of missing values per variable in the train set
	np.random.seed(0)
	m = 0.2
	for j in range(0, X_train.shape[1]):
		sub = np.random.choice(X_train.shape[0], int(X_train.shape[0]*m))
		X_train[sub, j] = np.nan

	# Train LCERegressor with default parameters
	reg = LCERegressor(n_jobs=-1, random_state=0)
	reg.fit(X_train, y_train)

	# Make prediction
	y_pred = reg.predict(X_test)
	mse = mean_squared_error(y_test, y_pred)
	print("The mean squared error (MSE) on test set: {:.0f}".format(mse))

.. code-block::
	
	The mean squared error (MSE) on test set: 3895
	

Python Source Files
-------------------


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCEClassifier on Iris Dataset">

.. only:: html

 .. figure:: _images/logo_lce.svg
     :alt: LCEClassifier on Iris dataset

     :ref:`sphx_glr_auto_examples_lceclassifier_iris.py`

.. raw:: html

    </div>

.. toctree::
   :hidden:

   /auto_examples/lceclassifier_iris
   
   

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCEClassifier on Iris Dataset with scikit-learn cross validation score">

.. only:: html

 .. figure:: _images/logo_lce.svg
     :alt: LCEClassifier on Iris dataset with scikit-learn cross validation score

     :ref:`sphx_glr_auto_examples_lceclassifier_iris_cv.py`

.. raw:: html

    </div>

.. toctree::
   :hidden:

   /auto_examples/lceclassifier_iris_cv
   


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCERegressor on Diabetes Dataset">

.. only:: html

 .. figure:: _images/logo_lce.svg
     :alt: LCERegressor on Diabetes dataset

     :ref:`sphx_glr_auto_examples_lceregressor_diabetes.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/lceregressor_diabetes
   
 
.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCERegressor with missing values">

.. only:: html

 .. figure:: _images/logo_lce.svg
     :alt: LCERegressor on Diabetes dataset with missing values

     :ref:`sphx_glr_auto_examples_lceregressor_missing_diabetes.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/lceregressor_missing_diabetes
   
   

.. raw:: html

    <div class="sphx-glr-clear"></div>



.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-gallery


  .. container:: sphx-glr-download sphx-glr-download-python

    :download:`Download all examples in Python source code: auto_examples_python.zip </auto_examples/auto_examples_python.zip>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

    :download:`Download all examples in Jupyter notebooks: auto_examples_jupyter.zip </auto_examples/auto_examples_jupyter.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
   